{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "There are three main types of missing data:\n",
    "- Missing completely at random (MCAR)\n",
    "- Missing at random (MAR)\n",
    "- Not missing at random (NMAR)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### MAR\n",
    "MAR data — means there is a systematic relationship between the propensity of missing values and the observed data, but not the missing data. What it means, is that the missingness of data can be predicted by other features in the dataset.\n",
    "In the table above you can see that the mileage has few missing values. If we analyze the dataset we can see that the manufacturing year of cars with missing values is lower than in other examples."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<img src=\"image.png\" height=200 width = 400/>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<img src=\"image2.png\" height= 200 width =400/>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Above, you can see the illustration. You can clearly see the correlation between the percentage of missing values in the mileage column and the manufacturing year of the car. From the illustration, we can assume that the older the car, the more the probability that the mileage will not be provided by the seller of the car. There can be a lot of reasons to not provide mileage for the old car, but the most important reason is — old cars have more mileage than the new ones. And the more the mileage is, the more the car has been used, which greatly affects its price. So, the long story short, we can predict the missingness of the mileage of the car, from its manufacturing year."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Missing Completely at Random (MCAR)\n",
    "MCAR means there is no relationship between the missingness of the data and any values, observed or missing. This kind of missing values is the easiest to understand. The fact that the data is missing has nothing to do neither with observed data nor with non-observed data, it’s just missing. There is no logic in it. From the illustration provided above, we can see that there is missing value in the color column, there can be a lot of reasons for its missingness, but not the systematic one. Someone just forgot to mention the color, someone was lazy to do it, maybe it was a problem with a system, nothing serious here."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Missing Not at Random (MNAR)\n",
    "MNAR data is the most complicated one both in terms of finding it and dealing with it. The fact that the data is missing is related to the unobserved data, i.e. the data that we don’t have, the missingness is related to factors that we didn’t account for. I know, that’s sad.\n",
    "The easiest way to understand why the data is missing is to understand the data collection process, also there are statistical methods to understand whether the data is MCAR or MAR."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Handling Missing Data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "__1- Do Nothing:__\n",
    "\n",
    "That’s an easy one. You just let the algorithm handle the missing data. Some algorithms can factor in the missing values and learn the best imputation values for the missing data based on the training loss reduction (ie. XGBoost). Some others have the option to just ignore them (ie. LightGBM — use_missing=false). However, other algorithms will panic and throw an error complaining about the missing values (ie. Scikit learn — LinearRegression). In that case, you will need to handle the missing data and clean it before feeding it to the algorithm.\n",
    "\n",
    "__2- Imputation Using (Mean/Median) Values:__\n",
    "This works by calculating the mean/median of the non-missing values in a column and then replacing the missing values within each column separately and independently from the others. It can only be used with numeric data.\n",
    "\n",
    "__Pros:__\n",
    "- Easy and fast.\n",
    "- Works well with small numerical datasets.\n",
    "\n",
    "__Cons:__\n",
    "- Doesn’t factor the correlations between features. It only works on the column level.\n",
    "- Will give poor results on encoded categorical features (do NOT use it on categorical features).\n",
    "- Not very accurate.\n",
    "- Doesn’t account for the uncertainty in the imputations.\n",
    "\n",
    "__3- Imputation Using (Most Frequent) or (Zero/Constant) Values:__\n",
    "Most Frequent is another statistical strategy to impute missing values and YES!! It works with categorical features (strings or numerical representations) by replacing missing data with the most frequent values within each column.\n",
    "\n",
    "__Pros:__\n",
    "- Works well with categorical features.\n",
    "\n",
    "__Cons:__\n",
    "- It also doesn’t factor the correlations between features.\n",
    "- It can introduce bias in the data.\n",
    "\n",
    "Zero or Constant imputation — as the name suggests — it replaces the missing values with either zero or any constant value you specify\n",
    "\n",
    "__4- Imputation Using k-NN:__\n",
    "The k nearest neighbours is an algorithm that is used for simple classification. The algorithm uses ‘feature similarity’ to predict the values of any new data points. This means that the new point is assigned a value based on how closely it resembles the points in the training set. This can be very useful in making predictions about the missing values by finding the k’s closest neighbours to the observation with missing data and then imputing them based on the non-missing values in the neighbourhood.\n",
    "\n",
    "How does it work?\n",
    "It creates a basic mean impute then uses the resulting complete list to construct a KDTree. Then, it uses the resulting KDTree to compute nearest neighbours (NN). After it finds the k-NNs, it takes the weighted average of them.\n",
    "Pros:\n",
    "Can be much more accurate than the mean, median or most frequent imputation methods (It depends on the dataset).\n",
    "Cons:\n",
    "Computationally expensive. KNN works by storing the whole training dataset in memory.\n",
    "\n",
    "K-NN is quite sensitive to outliers in the data (unlike SVM)\n",
    "\n",
    "__5- Imputation Using Multivariate Imputation by Chained Equation (MICE)__\n",
    "This type of imputation works by filling the missing data multiple times. Multiple Imputations (MIs) are much better than a single imputation as it measures the uncertainty of the missing values in a better way. The chained equations approach is also very flexible and can handle different variables of different data types (ie., continuous or binary) as well as complexities such as bounds or survey skip patterns.In Multiple Imputation, instead of imputing a single value for each missing entry we place there a set of values, which contain the natural variability. This method also uses predictive methods, but multiple times, creating different imputed datasets. Thereafter, created datasets analyzed and the single best dataset is created. This is a highly preferred method for data imputation, but moderately sophisticated\n",
    "\n",
    "__6- Imputation Using Deep Learning (Datawig):__\n",
    "This method works very well with categorical and non-numerical features. It is a library that learns Machine Learning models using Deep Neural Networks to impute missing values in a dataframe. It also supports both CPU and GPU for training."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
