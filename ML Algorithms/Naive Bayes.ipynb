{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The Naive Bayes classifier separates data into different classes according to the Bayes’ Theorem, along with the assumption that all the predictors are independent of one another. It assumes that a particular feature in a class is not related to the presence of other features."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "For example, you can consider a fruit to be a watermelon if it is green, round and has a 10-inch diameter. These features could depend on each other for their existence, but each one of them independently contributes to the probability that the fruit under consideration is a watermelon. That’s why this classifier has the term ‘Naive’ in its name. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This algorithm is quite popular because it can even outperform highly advanced classification techniques. Moreover, it’s quite simple, and you can build it quickly. \n",
    "\n",
    "Here’s the Bayes theorem, which is the basis for this algorithm:\n",
    "\n",
    "P(c | x) = P(x | c) P(c)/P(x)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In this equation, \n",
    "- ‘c’ stands for class. \n",
    "- ‘x’ stands for attributes.\n",
    "- P(c/x) stands for the posterior probability of class according to the predictor. \n",
    "- P(x) is the prior probability of the predictor.\n",
    "- P(c) is the prior probability of the class. \n",
    "- P(x/c) shows the probability of the predictor according to the class. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "__Advantages of Naive Bayes__\n",
    "- This algorithm works very fast and can easily predict the class of a test dataset. \n",
    "- You can use it to solve multi-class prediction problems as it’s quite useful with them. \n",
    "- Naive Bayes classifier performs better than other models with less training data if the assumption of independence of features holds. \n",
    "- If you have categorical input variables, the Naive Bayes algorithm performs exceptionally well in comparison to numerical variables. \n",
    "\n",
    "__Disadvantages of Naive Bayes__\n",
    "- If your test data set has a categorical variable of a category that wasn’t present in the training data set, the Naive Bayes model will assign it zero probability and won’t be able to make any predictions in this regard. This phenomenon is called ‘Zero Frequency,’ and you’ll have to use a smoothing technique to solve this problem.(alpha)\n",
    "- This algorithm is also notorious as a lousy estimator. So, you shouldn’t take the probability outputs of ‘predict_proba’ too seriously. \n",
    "- It assumes that all the features are independent. While it might sound great in theory, in real life, you’ll hardly find a set of independent features. \n",
    "\n",
    "__Applications of Naive Bayes Algorithm__\n",
    "As you must’ve noticed, this algorithm offers plenty of advantages to its users. That’s why it has a lot of applications in various sectors too. Here are some applications of Naive Bayes algorithm:\n",
    "\n",
    "- As this algorithm is fast and efficient, you can use it to make real-time predictions.\n",
    "- This algorithm is popular for multi-class predictions. You can find the probability of multiple target classes easily by using this algorithm.\n",
    "- Email services (like Gmail) use this algorithm to figure out whether an email is a spam or not. This algorithm is excellent for spam filtering.\n",
    "- Its assumption of feature independence, and its effectiveness in solving multi-class problems, makes it perfect for performing Sentiment Analysis. Sentiment Analysis refers to the identification of positive or negative sentiments of a target group (customers, audience, etc.)\n",
    "- Collaborative Filtering and the Naive Bayes algorithm work together to build recommendation systems. These systems use data mining and machine learning to predict if the user would like a particular resource or not."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "__Types of Naive Bayes Classifier__\n",
    "This algorithm has multiple kinds. Here are the main ones:\n",
    "\n",
    "- __Bernoulli Naive Bayes__\n",
    "Here, the predictors are boolean variables. So, the only values you have are ‘True’ and ‘False’ (you could also have ‘Yes’ or ‘No’). We use it when the data is according to multivariate Bernoulli distribution. Like MultinomialNB, this classifier is suitable for discrete data. The difference is that while MultinomialNB works with occurrence counts, BernoulliNB is designed for binary/boolean features.\n",
    "\n",
    "- __Multinomial Naive Bayes__\n",
    "People use this algorithm to solve document classification problems. For example, if you want to determine whether a document belongs to the ‘Legal’ category or ‘Human Resources’ category, you’d use this algorithm to sort it out. It uses the frequency of the present words as features. The multinomial Naive Bayes classifier is suitable for classification with discrete features (e.g., word counts for text classification). The multinomial distribution normally requires integer feature counts. However, in practice, fractional counts such as tf-idf may also work.\n",
    "\n",
    "- __Gaussian Naive Bayes__\n",
    "If the predictors aren’t discrete but have a continuous value, we assume that they are a sample from a gaussian distribution. \n",
    "\n",
    "- __ComplementNB__\n",
    "The Complement Naive Bayes classifier was designed to correct the “severe assumptions” made by the standard Multinomial Naive Bayes classifier. It is particularly suited for imbalanced data sets.\n",
    "\n",
    "- __CategoricalNB__\n",
    "CategoricalNB implements the categorical naive Bayes algorithm for categorically distributed data. It assumes that each feature, which is described by the index , has its own categorical distribution."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "__Out-of-core NB Model fitting__\n",
    "Naive Bayes models can be used to tackle large scale classification problems for which the full training set might not fit in memory. To handle this case, MultinomialNB, BernoulliNB, and GaussianNB expose a __partial_fit__ method that can be used incrementally as done with other classifiers as demonstrated in Out-of-core classification of text documents. All naive Bayes classifiers support sample weighting.\n",
    "\n",
    "Contrary to the fit method, the first call to partial_fit needs to be passed the list of all the expected class labels."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 1. Gaussian Naive Bayes\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\vischauh\\AppData\\Local\\conda\\conda\\envs\\packt\\lib\\importlib\\_bootstrap.py:219: RuntimeWarning: numpy.ufunc size changed, may indicate binary incompatibility. Expected 192 from C header, got 216 from PyObject\n",
      "  return f(*args, **kwds)\n"
     ]
    }
   ],
   "source": [
    "from sklearn.datasets import load_iris\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.naive_bayes import GaussianNB"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "X,y  = load_iris(return_X_y =True)\n",
    "X_train, X_test, y_train, y_test = train_test_split(X,y, test_size=0.5,\n",
    "                                                   random_state=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "gnb = GaussianNB()\n",
    "y_pred = gnb.fit(X_train,y_train).predict(X_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of mislabeled points out of a total 75 points: 4\n"
     ]
    }
   ],
   "source": [
    "print(f'Number of mislabeled points out of a total {X_test.shape[0]} points: {(y_test != y_pred).sum()}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['__abstractmethods__',\n",
       " '__class__',\n",
       " '__delattr__',\n",
       " '__dict__',\n",
       " '__dir__',\n",
       " '__doc__',\n",
       " '__eq__',\n",
       " '__format__',\n",
       " '__ge__',\n",
       " '__getattribute__',\n",
       " '__getstate__',\n",
       " '__gt__',\n",
       " '__hash__',\n",
       " '__init__',\n",
       " '__init_subclass__',\n",
       " '__le__',\n",
       " '__lt__',\n",
       " '__module__',\n",
       " '__ne__',\n",
       " '__new__',\n",
       " '__reduce__',\n",
       " '__reduce_ex__',\n",
       " '__repr__',\n",
       " '__setattr__',\n",
       " '__setstate__',\n",
       " '__sizeof__',\n",
       " '__str__',\n",
       " '__subclasshook__',\n",
       " '__weakref__',\n",
       " '_abc_cache',\n",
       " '_abc_negative_cache',\n",
       " '_abc_negative_cache_version',\n",
       " '_abc_registry',\n",
       " '_check_X',\n",
       " '_estimator_type',\n",
       " '_get_param_names',\n",
       " '_get_tags',\n",
       " '_joint_log_likelihood',\n",
       " '_more_tags',\n",
       " '_partial_fit',\n",
       " '_update_mean_variance',\n",
       " 'class_count_',\n",
       " 'class_prior_',\n",
       " 'classes_',\n",
       " 'epsilon_',\n",
       " 'fit',\n",
       " 'get_params',\n",
       " 'partial_fit',\n",
       " 'predict',\n",
       " 'predict_log_proba',\n",
       " 'predict_proba',\n",
       " 'priors',\n",
       " 'score',\n",
       " 'set_params',\n",
       " 'sigma_',\n",
       " 'theta_',\n",
       " 'var_smoothing']"
      ]
     },
     "execution_count": 28,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dir(gnb)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 2. MultiNominalNB"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "rng = np.random.RandomState(1)\n",
    "X = rng.randint(5, size=(6, 100))\n",
    "y = np.array([1, 2, 3, 4, 5, 6])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(6, 100)"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[3]\n"
     ]
    }
   ],
   "source": [
    "from sklearn.naive_bayes import MultinomialNB\n",
    "clf = MultinomialNB()\n",
    "clf.fit(X, y)\n",
    "print(clf.predict(X[2:3]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['__abstractmethods__',\n",
       " '__class__',\n",
       " '__delattr__',\n",
       " '__dict__',\n",
       " '__dir__',\n",
       " '__doc__',\n",
       " '__eq__',\n",
       " '__format__',\n",
       " '__ge__',\n",
       " '__getattribute__',\n",
       " '__getstate__',\n",
       " '__gt__',\n",
       " '__hash__',\n",
       " '__init__',\n",
       " '__init_subclass__',\n",
       " '__le__',\n",
       " '__lt__',\n",
       " '__module__',\n",
       " '__ne__',\n",
       " '__new__',\n",
       " '__reduce__',\n",
       " '__reduce_ex__',\n",
       " '__repr__',\n",
       " '__setattr__',\n",
       " '__setstate__',\n",
       " '__sizeof__',\n",
       " '__str__',\n",
       " '__subclasshook__',\n",
       " '__weakref__',\n",
       " '_abc_cache',\n",
       " '_abc_negative_cache',\n",
       " '_abc_negative_cache_version',\n",
       " '_abc_registry',\n",
       " '_check_X',\n",
       " '_check_X_y',\n",
       " '_check_alpha',\n",
       " '_count',\n",
       " '_estimator_type',\n",
       " '_get_coef',\n",
       " '_get_intercept',\n",
       " '_get_param_names',\n",
       " '_get_tags',\n",
       " '_init_counters',\n",
       " '_joint_log_likelihood',\n",
       " '_more_tags',\n",
       " '_update_class_log_prior',\n",
       " '_update_feature_log_prob',\n",
       " 'alpha',\n",
       " 'class_count_',\n",
       " 'class_log_prior_',\n",
       " 'class_prior',\n",
       " 'classes_',\n",
       " 'coef_',\n",
       " 'feature_all_',\n",
       " 'feature_count_',\n",
       " 'feature_log_prob_',\n",
       " 'fit',\n",
       " 'fit_prior',\n",
       " 'get_params',\n",
       " 'intercept_',\n",
       " 'n_features_',\n",
       " 'norm',\n",
       " 'partial_fit',\n",
       " 'predict',\n",
       " 'predict_log_proba',\n",
       " 'predict_proba',\n",
       " 'score',\n",
       " 'set_params']"
      ]
     },
     "execution_count": 27,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dir(clf)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 3. BernoulliNB"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[3]\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "rng = np.random.RandomState(1)\n",
    "X = rng.randint(5, size=(6, 100))\n",
    "Y = np.array([1, 2, 3, 4, 4, 5])\n",
    "from sklearn.naive_bayes import BernoulliNB\n",
    "clf = BernoulliNB()\n",
    "clf.fit(X, Y)\n",
    "print(clf.predict(X[2:3]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['__abstractmethods__',\n",
       " '__class__',\n",
       " '__delattr__',\n",
       " '__dict__',\n",
       " '__dir__',\n",
       " '__doc__',\n",
       " '__eq__',\n",
       " '__format__',\n",
       " '__ge__',\n",
       " '__getattribute__',\n",
       " '__getstate__',\n",
       " '__gt__',\n",
       " '__hash__',\n",
       " '__init__',\n",
       " '__init_subclass__',\n",
       " '__le__',\n",
       " '__lt__',\n",
       " '__module__',\n",
       " '__ne__',\n",
       " '__new__',\n",
       " '__reduce__',\n",
       " '__reduce_ex__',\n",
       " '__repr__',\n",
       " '__setattr__',\n",
       " '__setstate__',\n",
       " '__sizeof__',\n",
       " '__str__',\n",
       " '__subclasshook__',\n",
       " '__weakref__',\n",
       " '_abc_cache',\n",
       " '_abc_negative_cache',\n",
       " '_abc_negative_cache_version',\n",
       " '_abc_registry',\n",
       " '_check_X',\n",
       " '_check_X_y',\n",
       " '_check_alpha',\n",
       " '_count',\n",
       " '_estimator_type',\n",
       " '_get_coef',\n",
       " '_get_intercept',\n",
       " '_get_param_names',\n",
       " '_get_tags',\n",
       " '_init_counters',\n",
       " '_joint_log_likelihood',\n",
       " '_more_tags',\n",
       " '_update_class_log_prior',\n",
       " '_update_feature_log_prob',\n",
       " 'alpha',\n",
       " 'class_count_',\n",
       " 'class_log_prior_',\n",
       " 'class_prior',\n",
       " 'classes_',\n",
       " 'coef_',\n",
       " 'feature_all_',\n",
       " 'feature_count_',\n",
       " 'feature_log_prob_',\n",
       " 'fit',\n",
       " 'fit_prior',\n",
       " 'get_params',\n",
       " 'intercept_',\n",
       " 'n_features_',\n",
       " 'norm',\n",
       " 'partial_fit',\n",
       " 'predict',\n",
       " 'predict_log_proba',\n",
       " 'predict_proba',\n",
       " 'score',\n",
       " 'set_params']"
      ]
     },
     "execution_count": 26,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dir(clf)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 4. ComplementNB"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([3])"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from sklearn.naive_bayes import ComplementNB\n",
    "clf = ComplementNB()\n",
    "clf.fit(X,y).predict(X[2:3])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['__abstractmethods__',\n",
       " '__class__',\n",
       " '__delattr__',\n",
       " '__dict__',\n",
       " '__dir__',\n",
       " '__doc__',\n",
       " '__eq__',\n",
       " '__format__',\n",
       " '__ge__',\n",
       " '__getattribute__',\n",
       " '__getstate__',\n",
       " '__gt__',\n",
       " '__hash__',\n",
       " '__init__',\n",
       " '__init_subclass__',\n",
       " '__le__',\n",
       " '__lt__',\n",
       " '__module__',\n",
       " '__ne__',\n",
       " '__new__',\n",
       " '__reduce__',\n",
       " '__reduce_ex__',\n",
       " '__repr__',\n",
       " '__setattr__',\n",
       " '__setstate__',\n",
       " '__sizeof__',\n",
       " '__str__',\n",
       " '__subclasshook__',\n",
       " '__weakref__',\n",
       " '_abc_cache',\n",
       " '_abc_negative_cache',\n",
       " '_abc_negative_cache_version',\n",
       " '_abc_registry',\n",
       " '_check_X',\n",
       " '_check_X_y',\n",
       " '_check_alpha',\n",
       " '_count',\n",
       " '_estimator_type',\n",
       " '_get_coef',\n",
       " '_get_intercept',\n",
       " '_get_param_names',\n",
       " '_get_tags',\n",
       " '_init_counters',\n",
       " '_joint_log_likelihood',\n",
       " '_more_tags',\n",
       " '_update_class_log_prior',\n",
       " '_update_feature_log_prob',\n",
       " 'alpha',\n",
       " 'class_count_',\n",
       " 'class_log_prior_',\n",
       " 'class_prior',\n",
       " 'classes_',\n",
       " 'coef_',\n",
       " 'feature_all_',\n",
       " 'feature_count_',\n",
       " 'feature_log_prob_',\n",
       " 'fit',\n",
       " 'fit_prior',\n",
       " 'get_params',\n",
       " 'intercept_',\n",
       " 'n_features_',\n",
       " 'norm',\n",
       " 'partial_fit',\n",
       " 'predict',\n",
       " 'predict_log_proba',\n",
       " 'predict_proba',\n",
       " 'score',\n",
       " 'set_params']"
      ]
     },
     "execution_count": 25,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dir(clf)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 5. CategoricalNB"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([3])"
      ]
     },
     "execution_count": 29,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from sklearn.naive_bayes import CategoricalNB\n",
    "clf = CategoricalNB()\n",
    "clf.fit(X,y).predict(X[2:3])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['__abstractmethods__',\n",
       " '__class__',\n",
       " '__delattr__',\n",
       " '__dict__',\n",
       " '__dir__',\n",
       " '__doc__',\n",
       " '__eq__',\n",
       " '__format__',\n",
       " '__ge__',\n",
       " '__getattribute__',\n",
       " '__getstate__',\n",
       " '__gt__',\n",
       " '__hash__',\n",
       " '__init__',\n",
       " '__init_subclass__',\n",
       " '__le__',\n",
       " '__lt__',\n",
       " '__module__',\n",
       " '__ne__',\n",
       " '__new__',\n",
       " '__reduce__',\n",
       " '__reduce_ex__',\n",
       " '__repr__',\n",
       " '__setattr__',\n",
       " '__setstate__',\n",
       " '__sizeof__',\n",
       " '__str__',\n",
       " '__subclasshook__',\n",
       " '__weakref__',\n",
       " '_abc_cache',\n",
       " '_abc_negative_cache',\n",
       " '_abc_negative_cache_version',\n",
       " '_abc_registry',\n",
       " '_check_X',\n",
       " '_check_X_y',\n",
       " '_check_alpha',\n",
       " '_count',\n",
       " '_estimator_type',\n",
       " '_get_coef',\n",
       " '_get_intercept',\n",
       " '_get_param_names',\n",
       " '_get_tags',\n",
       " '_init_counters',\n",
       " '_joint_log_likelihood',\n",
       " '_more_tags',\n",
       " '_update_class_log_prior',\n",
       " '_update_feature_log_prob',\n",
       " 'alpha',\n",
       " 'category_count_',\n",
       " 'class_count_',\n",
       " 'class_log_prior_',\n",
       " 'class_prior',\n",
       " 'classes_',\n",
       " 'coef_',\n",
       " 'feature_log_prob_',\n",
       " 'fit',\n",
       " 'fit_prior',\n",
       " 'get_params',\n",
       " 'intercept_',\n",
       " 'n_features_',\n",
       " 'partial_fit',\n",
       " 'predict',\n",
       " 'predict_log_proba',\n",
       " 'predict_proba',\n",
       " 'score',\n",
       " 'set_params']"
      ]
     },
     "execution_count": 30,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dir(clf)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
