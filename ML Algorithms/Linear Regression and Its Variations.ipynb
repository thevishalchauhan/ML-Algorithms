{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "kernelspec": {
      "display_name": "Python 3",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.6.10"
    },
    "colab": {
      "name": "Linear Regression and Its Variations.ipynb",
      "provenance": []
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "e_zXSnyQByL4"
      },
      "source": [
        "__Linear Regression__\n",
        "is an attractive model because the representation is so simple. The representation is a linear equation that combines a specific set of input values (x) the solution to which is\n",
        "the predicted output for that set of input values (y). As such, both the input values (x) and\n",
        "the output value are numeric.\n",
        "The linear equation assigns one scale factor to each input value or column, called a coeficient\n",
        "that is commonly represented by the Greek letter Beta. One additional coeficient is\n",
        "also added, giving the line an additional degree of freedom (e.g. moving up and down on a\n",
        "two-dimensional plot) and is often called the intercept or the bias coeficient. For example, in a\n",
        "simple regression problem (a single x and a single y), the form of the model would be:\n",
        "y = B0 + B1 * x"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ZXzdJmvsByL-"
      },
      "source": [
        "### Assumptions\n",
        "\n",
        "- __Linear Assumption__. Linear regression assumes that the relationship between your input\n",
        "and output is linear. It does not support anything else. This may be obvious, but it is\n",
        "good to remember when you have a lot of attributes. You may need to transform data to\n",
        "make the relationship linear (e.g. log transform for an exponential relationship).\n",
        "- __Remove Noise__. Linear regression assumes that your input and output variables are\n",
        "not noisy. Consider using data cleaning operations that let you better expose and clarify\n",
        "the signal in your data. This is most important for the output variable and you want to\n",
        "remove outliers in the output variable (y) if possible.\n",
        "- __Remove Collinearity__. Linear regression will over-fit your data when you have highly correlated input variables. Consider calculating pairwise correlations for your input data and removing the most correlated.\n",
        "- __Gaussian Distributions__. Linear regression will make more reliable predictions if your\n",
        "input and output variables have a Gaussian distribution. You may get some benefit using\n",
        "transforms (e.g. log or BoxCox) on you variables to make their distribution more Gaussian\n",
        "looking.\n",
        "- __Rescale Inputs__: Linear regression will often make more reliable predictions if you rescale\n",
        "input variables using standardization or normalization."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "5wYKuyauByL_"
      },
      "source": [
        "B1 = corr(x; y) *stdev(y)/stdev(x)"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "JYgt7zKKByL_"
      },
      "source": [
        "def linear_regression(x,y,iterations=100,\n",
        "                      learning_rate=0.01):\n",
        "  n,m= len(x[0]),len(x)\n",
        "  beta_0, beta_other = initialize_params(n)\n",
        "  for _ in range(iterations):\n",
        "    gradient_beta_0, gradient_beta_other = compute_gradient(\n",
        "        x,y,beta_0,beta_other,n,m\n",
        "    )\n",
        "    beta_0, beta_other = update_params(\n",
        "        beta_0,beta_other, gradient_beta_0, gradient_beta_other,\n",
        "        learning_rate\n",
        "    )\n",
        "  return beta_0, beta_other"
      ],
      "execution_count": 1,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "eBu1KdaiCvD7"
      },
      "source": [
        "import random\n",
        "def initialize_params(dimensions):\n",
        "  beta_0 = 0\n",
        "  beta_other = [random.random() for i in range(dimensions)]\n",
        "  return beta_0, beta_other"
      ],
      "execution_count": 3,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "3mkd4k9JDJhP"
      },
      "source": [
        "def compute_gradient(x,y,beta_0,beta_other,dimension,m):\n",
        "  gradient_beta_0 = 0\n",
        "  gradient_beta_other = [0] * dimension\n",
        "\n",
        "  for i in range(m):\n",
        "    y_i_hat = sum(x[i][j] * beta_other[j] for j in range(dimension))+ beta_0\n",
        "    derror_dy = 2 * (y[i] - y_i_hat)\n",
        "    for j in range(dimension):\n",
        "      gradient_beta_other[j] +=derror_dy + x[i][j]/m\n",
        "    gradient_beta_0 += derror_dy/m\n",
        "  return gradient_beta_0, gradient_beta_other"
      ],
      "execution_count": 5,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Iskh-GVmERlC"
      },
      "source": [
        "def update_params(beta_0, beta_other, gradient_beta_0,\n",
        "                  gradient_beta_other, learning_rate):\n",
        "  beta_0 += gradient_beta_0 * learning_rate\n",
        "  for i in range(len(beta_other)):\n",
        "    beta_other[i] +=(gradient_beta_other[i] * learning_rate)\n",
        "  return beta_0, beta_other"
      ],
      "execution_count": 6,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ks1ALtPAFqJ5"
      },
      "source": [
        ""
      ],
      "execution_count": null,
      "outputs": []
    }
  ]
}