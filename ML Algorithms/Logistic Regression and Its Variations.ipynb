{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "kernelspec": {
      "display_name": "Python 3",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.6.10"
    },
    "colab": {
      "name": "Logistic Regression and Its Variations.ipynb",
      "provenance": []
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "2OjlMivPMb72"
      },
      "source": [
        "## 1. Logistic Regression"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "NYVTeeauMb7_"
      },
      "source": [
        "**The Similarities between Linear Regression and Logistic Regression**\n",
        "- Linear Regression and Logistic Regression both are supervised Machine Learning algorithms.\n",
        "- Linear Regression and Logistic Regression, both the models are parametric regression i.e. both the models use linear equations for predictions\n",
        "\n",
        "**The Differences between Linear Regression and Logistic Regression**\n",
        "\n",
        "- Linear Regression is used to handle regression problems whereas Logistic regression is used to handle the classification problems.\n",
        "- Linear regression provides a continuous output but Logistic regression provides discreet output.\n",
        "- The purpose of Linear Regression is to find the best-fitted line while Logistic regression is one step ahead and fitting the line values to the sigmoid curve.\n",
        "- The method for calculating loss function in linear regression is the mean squared error whereas for logistic regression it is maximum likelihood estimation."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "vKxdvFCHMb8C"
      },
      "source": [
        ""
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "k1AiQoYTMb8D"
      },
      "source": [
        "__Logistic regression__ is a classification algorithm traditionally limited to only two-class classification problems.\n",
        "\n",
        "If you have more than two classes then __Linear Discriminant Analysis__ is the preferred linear classification technique."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "0e_ObCsvMb8E"
      },
      "source": [
        "__Limitations of Logistic Regression__\n",
        "Logistic regression is a simple and powerful linear classification algorithm. It also has limitations that suggest at the need for alternate linear classification algorithms.\n",
        "\n",
        "- __Two-Class Problems__: Logistic regression is intended for two-class or binary classification problems. It can be extended for multi-class classification, but is rarely used for this purpose.\n",
        "- __Unstable With Well Separated Classes__: Logistic regression can become unstable when the classes are well separated.\n",
        "- __Unstable With Few Example__: Logistic regression can become unstable when there are few examples from which to estimate the parameters."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "2xvyV-RuMb8F"
      },
      "source": [
        "def logistic_regression(x,y,iterations=100,learning_rate=0.01):\n",
        "  m,n = len(x), len(x[0])\n",
        "  beta_0,beta_other = initialize_params(n)\n",
        "  for _ in range(iterations):\n",
        "    gradient_beta_0,gradient_beta_other = compute_gradients(x,y,\n",
        "                                                            beta_0,\n",
        "                                                            beta_other,\n",
        "                                                            m,n)\n",
        "    beta_0, beta_other = update_params(beta_0,beta_other,\n",
        "                                       gradient_beta_0,\n",
        "                                       gradient_beta_other,\n",
        "                                       learning_rate)\n",
        "  return beta_0, beta_other"
      ],
      "execution_count": 3,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "HKTocCmJNMs0"
      },
      "source": [
        "import random\n",
        "def initialize_params(n):\n",
        "  beta_0 = 0\n",
        "  beta_other = [random.random() for _ in range(n)]\n",
        "  return beta_0, beta_other"
      ],
      "execution_count": 4,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "AOxT5pPGNk2v"
      },
      "source": [
        "def computer_gradients(x,y,beta_0,beta_other,m,n):\n",
        "  gradient_beta_0 = 0\n",
        "  gradient_other = [0] * n\n",
        "\n",
        "  for i, point in enumerate(x):\n",
        "    pred = logistic_function(point,beta_0,beta_other)\n",
        "\n",
        "    for j, feature in enumerate(point):\n",
        "      gradient_beta_other[j] += (pred - y[i]) * feature/m\n",
        "    gradient_beta_0 += (pred - y[i])/m\n",
        "  return gradient_beta_0, gradient_beta_other"
      ],
      "execution_count": 5,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Gw5n2ORbOXNf"
      },
      "source": [
        "import numpy as np\n",
        "def logistic_function(point, beta_0,beta_other):\n",
        "  return 1/(1+np.exp(-(beta_0 + point.dot(beta_other))))"
      ],
      "execution_count": 6,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "MTeIDeBLOqIa"
      },
      "source": [
        "def update_params(beta_0, beta_other, gradient_beta_0, gradient_beta_other,\n",
        "                  learning_rate):\n",
        "  beta_0 -= gradient_beta_0 * learning_rate\n",
        "  for i in range(len(beta_other)):\n",
        "    beta_other[i] -= gradient_beta_other[i] * learning_rate\n",
        "  return beta_0, beta_other"
      ],
      "execution_count": 7,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "gWaIpcAVPQqu"
      },
      "source": [
        ""
      ],
      "execution_count": null,
      "outputs": []
    }
  ]
}